import streamlit as st
import cv2
import numpy as np
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase
import mediapipe as mp
import av
from collections import deque

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MediaPipe Pose
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils

class PoseProcessor(VideoProcessorBase):
    def __init__(self):
        self.pose = mp_pose.Pose(min_detection_confidence=0.5,
                                min_tracking_confidence=0.5,
                                model_complexity=1)
        self.status_deque = deque(maxlen=1)  # –î–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ —Ç–µ–∫—Å—Ç–∞ –≤ UI

    def analyze_posture(self, landmarks, image_shape):
        h, w, _ = image_shape
        left_shoulder = landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]
        right_shoulder = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]
        left_hip = landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP]
        right_hip = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP]
        left_ear = landmarks.landmark[mp_pose.PoseLandmark.LEFT_EAR]
        right_ear = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EAR]
        nose = landmarks.landmark[mp_pose.PoseLandmark.NOSE]

        sitting = left_hip.y < left_shoulder.y + 0.1 or right_hip.y < right_shoulder.y + 0.1

        messages = []

        head_forward = (left_ear.y > left_shoulder.y + 0.1 or right_ear.y > right_shoulder.y + 0.1) and \
                       (nose.y > left_shoulder.y or nose.y > right_shoulder.y)
        if head_forward:
            messages.append("‚Ä¢ –ì–æ–ª–æ–≤–∞ –Ω–∞–∫–ª–æ–Ω–µ–Ω–∞ –≤–ø–µ—Ä–µ–¥ (—Ç–µ–∫—Å—Ç–æ–≤–∞—è —à–µ—è)")

        shoulders_rounded = left_shoulder.x > left_hip.x + 0.05 or right_shoulder.x < right_hip.x - 0.05
        if shoulders_rounded:
            messages.append("‚Ä¢ –ü–ª–µ—á–∏ —Å—Å—É—Ç—É–ª–µ–Ω—ã (–æ–∫—Ä—É–≥–ª–µ–Ω—ã –≤–ø–µ—Ä–µ–¥)")

        shoulder_diff = abs(left_shoulder.y - right_shoulder.y)
        hip_diff = abs(left_hip.y - right_hip.y)
        if shoulder_diff > 0.05 or hip_diff > 0.05:
            messages.append("‚Ä¢ –ù–∞–∫–ª–æ–Ω –≤ —Å—Ç–æ—Ä–æ–Ω—É (–Ω–µ—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –æ—Å–∞–Ω–∫–∞)")

        if sitting and (left_hip.y < left_shoulder.y + 0.15 or right_hip.y < right_shoulder.y + 0.15):
            messages.append("‚Ä¢ –¢–∞–∑ –Ω–∞–∫–ª–æ–Ω–µ–Ω –≤–ø–µ—Ä–µ–¥ (—Å–∏–¥—è)")

        if messages:
            report = [
                f"**{'–°–∏–¥—è' if sitting else '–°—Ç–æ—è'} - –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã:**",
                *messages,
                "\n**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**",
                "‚Ä¢ –î–µ—Ä–∂–∏—Ç–µ –≥–æ–ª–æ–≤—É –ø—Ä—è–º–æ, —É—à–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞–¥ –ø–ª–µ—á–∞–º–∏",
                "‚Ä¢ –û—Ç–≤–µ–¥–∏—Ç–µ –ø–ª–µ—á–∏ –Ω–∞–∑–∞–¥ –∏ –≤–Ω–∏–∑",
                "‚Ä¢ –î–µ—Ä–∂–∏—Ç–µ —Å–ø–∏–Ω—É –ø—Ä—è–º–æ–π, –∏–∑–±–µ–≥–∞–π—Ç–µ –Ω–∞–∫–ª–æ–Ω–æ–≤ –≤ —Å—Ç–æ—Ä–æ–Ω—ã",
                "‚Ä¢ –ü—Ä–∏ —Å–∏–¥–µ–Ω–∏–∏ –æ–ø–∏—Ä–∞–π—Ç–µ—Å—å –Ω–∞ —Å–µ–¥–∞–ª–∏—â–Ω—ã–µ –±—É–≥—Ä—ã"
            ]
        else:
            report = [
                f"**–û—Ç–ª–∏—á–Ω–∞—è –æ—Å–∞–Ω–∫–∞ ({'—Å–∏–¥—è' if sitting else '—Å—Ç–æ—è'})!**",
                "–í—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ–ª–æ–∂–µ–Ω–∏–∏.",
                "\n**–°–æ–≤–µ—Ç:**",
                "‚Ä¢ –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Å–ª–µ–¥–∏—Ç—å –∑–∞ –æ—Å–∞–Ω–∫–æ–π –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–Ω—è"
            ]

        return "\n\n".join(report)

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        results = self.pose.process(img_rgb)
        if results.pose_landmarks:
            # –†–∏—Å—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏
            mp_drawing.draw_landmarks(
                img, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),
                mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)
            )
            status = self.analyze_posture(results.pose_landmarks, img.shape)
            self.status_deque.append(status)
        else:
            self.status_deque.append("–ö–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")

        return av.VideoFrame.from_ndarray(img, format="bgr24")

def main():
    st.set_page_config(layout="wide")
    st.title("üì∑ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –æ—Å–∞–Ω–∫–∏ —Å –≤–µ–±-–∫–∞–º–µ—Ä—ã (WebRTC)")
    st.write("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∞—à—É –æ—Å–∞–Ω–∫—É –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –ø–æ–º–æ—â—å—é –∫–∞–º–µ—Ä—ã –±—Ä–∞—É–∑–µ—Ä–∞.")

    # –ó–∞–ø—É—Å–∫–∞–µ–º WebRTC —Å—Ç—Ä–∏–º–µ—Ä —Å –Ω–∞—à–∏–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º
    webrtc_ctx = webrtc_streamer(
        key="pose-analyzer",
        video_processor_factory=PoseProcessor,
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True
    )

    # –í—ã–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –∏–∑ –ø—Ä–æ—Ü–µ—Å—Å–∞
    if webrtc_ctx.video_processor:
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–æ—Å—Ç—É–ø–Ω—É—é —Å—Ç—Ä–æ–∫—É —Å –∞–Ω–∞–ª–∏–∑–æ–º
        if webrtc_ctx.video_processor.status_deque:
            analysis_text = webrtc_ctx.video_processor.status_deque[-1]
        else:
            analysis_text = "–û–∂–∏–¥–∞–Ω–∏–µ –≤–∏–¥–µ–æ –∏ –∞–Ω–∞–ª–∏–∑–∞..."

        st.markdown("### –ê–Ω–∞–ª–∏–∑ –æ—Å–∞–Ω–∫–∏:")
        st.markdown(analysis_text)

if __name__ == "__main__":
    main()