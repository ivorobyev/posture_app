import streamlit as st
import cv2
import numpy as np
from ultralytics import YOLO
import time
from enum import Enum

class PostureType(Enum):
    SITTING = "—Å–∏–¥—è"
    STANDING = "—Å—Ç–æ—è"
    UNKNOWN = "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"

@st.cache_resource
def load_model():
    return YOLO('yolov8n-pose.pt')

model = load_model()

def get_keypoint_safe(keypoints, idx, fallback=None):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ—á–∫–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π confidence"""
    if keypoints is None or idx >= len(keypoints):
        return fallback if fallback is not None else np.array([0, 0, 0])
    
    point = keypoints[idx]
    if len(point) >= 3 and point[2] > 0.25:  # –ü–æ–Ω–∏–∂–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤
        return point[:3]
    return fallback if fallback is not None else np.array([0, 0, 0])

def calculate_angle(a, b, c):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —É–≥–ª–∞ –º–µ–∂–¥—É —Ç—Ä–µ–º—è —Ç–æ—á–∫–∞–º–∏"""
    a, b, c = np.array(a), np.array(b), np.array(c)
    ba = a - b
    bc = c - b
    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))
    return np.degrees(np.arccos(np.clip(cosine_angle, -1, 1)))

def determine_posture_type(keypoints, img_height):
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–∑—ã —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –¥–ª—è —Å–∏–¥—è—á–µ–≥–æ –ø–æ–ª–æ–∂–µ–Ω–∏—è"""
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è —Å–∏–¥—è—á–µ–≥–æ –ø–æ–ª–æ–∂–µ–Ω–∏—è
    LEFT_SHOULDER = 5
    RIGHT_SHOULDER = 6
    LEFT_HIP = 11
    RIGHT_HIP = 12
    
    ls = get_keypoint_safe(keypoints, LEFT_SHOULDER)
    rs = get_keypoint_safe(keypoints, RIGHT_SHOULDER)
    lh = get_keypoint_safe(keypoints, LEFT_HIP)
    rh = get_keypoint_safe(keypoints, RIGHT_HIP)
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è —Å–∏–¥—è—á–µ–≥–æ –ø–æ–ª–æ–∂–µ–Ω–∏—è (–∫–æ–≥–¥–∞ –±–µ–¥—Ä–∞ –Ω–µ –≤–∏–¥–Ω—ã)
    if lh[2] < 0.3 or rh[2] < 0.3:
        # –ï—Å–ª–∏ –±–µ–¥—Ä–∞ –Ω–µ –≤–∏–¥–Ω—ã, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–∏–¥–∏—Ç (–æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ü–µ–Ω–∞—Ä–∏–π)
        return PostureType.SITTING
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏, –µ—Å–ª–∏ –±–µ–¥—Ä–∞ –≤–∏–¥–Ω—ã
    shoulder_hip_ratio = ((ls[1] - lh[1]) + (rs[1] - rh[1])) / (2 * img_height)
    if shoulder_hip_ratio < 0.15:  # –ü–ª–µ—á–∏ –±–ª–∏–∑–∫–æ –∫ –±–µ–¥—Ä–∞–º
        return PostureType.SITTING
    
    return PostureType.STANDING

def analyze_sitting_posture(keypoints, img_size):
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Å–∏–¥—è—á–µ–≥–æ –ø–æ–ª–æ–∂–µ–Ω–∏—è"""
    NOSE = 0
    LEFT_EAR = 3
    RIGHT_EAR = 4
    LEFT_SHOULDER = 5
    RIGHT_SHOULDER = 6
    LEFT_ELBOW = 7
    RIGHT_ELBOW = 8
    
    img_h, img_w = img_size
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ—á–∫–∏ –≤–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç–∏ —Ç–µ–ª–∞
    ls = get_keypoint_safe(keypoints, LEFT_SHOULDER)
    rs = get_keypoint_safe(keypoints, RIGHT_SHOULDER)
    le = get_keypoint_safe(keypoints, LEFT_EAR, ls)
    re = get_keypoint_safe(keypoints, RIGHT_EAR, rs)
    nose = get_keypoint_safe(keypoints, NOSE, (ls + rs) / 2)
    lelb = get_keypoint_safe(keypoints, LEFT_ELBOW)
    relb = get_keypoint_safe(keypoints, RIGHT_ELBOW)
    
    messages = []
    
    # 1. –ê–Ω–∞–ª–∏–∑ –≥–æ–ª–æ–≤—ã (–æ—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–∏ —Å–∏–¥–µ–Ω–∏–∏)
    head_forward = False
    if le[2] > 0.25 and re[2] > 0.25:  # –ï—Å–ª–∏ –≤–∏–¥–Ω—ã –æ–±–∞ —É—Ö–∞
        head_angle = calculate_angle(le[:2], nose[:2], re[:2])
        if head_angle < 150:  # –ù–∞–∫–ª–æ–Ω –≥–æ–ª–æ–≤—ã –≤–ø–µ—Ä–µ–¥
            head_forward = True
    elif nose[2] > 0.3:
        if nose[1] > (ls[1] + rs[1])/2 + 0.12*img_h:  # –ù–æ—Å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∏–∂–µ –ø–ª–µ—á
            head_forward = True
    
    if head_forward:
        messages.append("‚Ä¢ –ì–æ–ª–æ–≤–∞ –Ω–∞–∫–ª–æ–Ω–µ–Ω–∞ –≤–ø–µ—Ä–µ–¥ (—Ç–µ–∫—Å—Ç–æ–≤–∞—è —à–µ—è)")
    
    # 2. –ê–Ω–∞–ª–∏–∑ –ø–ª–µ—á –∏ –ª–æ–∫—Ç–µ–π (–¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Å—É—Ç—É–ª–æ—Å—Ç–∏)
    if lelb[2] > 0.3 and relb[2] > 0.3:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞, –µ—Å–ª–∏ –ª–æ–∫—Ç–∏ –ø–µ—Ä–µ–¥ –ø–ª–µ—á–∞–º–∏ (—Å—É—Ç—É–ª–æ—Å—Ç—å)
        if (lelb[0] > ls[0] + 0.05*img_w) or (relb[0] < rs[0] - 0.05*img_w):
            messages.append("‚Ä¢ –ü–ª–µ—á–∏ —Å—Å—É—Ç—É–ª–µ–Ω—ã (–ª–æ–∫—Ç–∏ –≤—ã–¥–≤–∏–Ω—É—Ç—ã –≤–ø–µ—Ä–µ–¥)")
    else:
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –ø–æ–ª–æ–∂–µ–Ω–∏—é –ø–ª–µ—á
        shoulder_angle = calculate_angle(ls[:2], (ls+rs)[:2]/2, rs[:2])
        if shoulder_angle < 160:  # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ –ø–ª–µ—á
            messages.append("‚Ä¢ –ü–ª–µ—á–∏ —Å–≤–µ–¥–µ–Ω—ã –≤–ø–µ—Ä–µ–¥")
    
    # 3. –ê–Ω–∞–ª–∏–∑ –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏ (—Å–∫–æ–ª–∏–æ–∑)
    if ls[2] > 0.3 and rs[2] > 0.3:
        shoulder_diff = abs(ls[1] - rs[1]) / img_h
        if shoulder_diff > 0.08:  # –†–∞–∑–Ω–∏—Ü–∞ –≤—ã—Å–æ—Ç—ã –ø–ª–µ—á
            messages.append("‚Ä¢ –ê—Å–∏–º–º–µ—Ç—Ä–∏—è –ø–ª–µ—á (–≤–æ–∑–º–æ–∂–µ–Ω —Å–∫–æ–ª–∏–æ–∑)")
    
    return messages

def analyze_posture(image):
    """–ê–Ω–∞–ª–∏–∑ –æ—Å–∞–Ω–∫–∏ —Å —É–ø–æ—Ä–æ–º –Ω–∞ —Å–∏–¥—è—á–µ–µ –ø–æ–ª–æ–∂–µ–Ω–∏–µ"""
    try:
        results = model(image, verbose=False)
        
        if not results or len(results) == 0 or results[0].keypoints is None:
            return image, "–í—Å—Ç–∞–Ω—å—Ç–µ –ø–µ—Ä–µ–¥ –∫–∞–º–µ—Ä–æ–π", PostureType.UNKNOWN
        
        keypoints = results[0].keypoints.xy[0].cpu().numpy()
        annotated_image = results[0].plot()
        
        if len(keypoints) < 13:  # –ú–∏–Ω–∏–º—É–º: –≥–æ–ª–æ–≤–∞, –ø–ª–µ—á–∏, –ª–æ–∫—Ç–∏
            return annotated_image, "–ü–æ–≤–µ—Ä–Ω–∏—Ç–µ—Å—å –∫ –∫–∞–º–µ—Ä–µ –≤–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç—å—é —Ç–µ–ª–∞", PostureType.UNKNOWN
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑—É (—Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Å–∏–¥—è—á–µ–≥–æ –ø–æ–ª–æ–∂–µ–Ω–∏—è)
        posture_type = determine_posture_type(keypoints, image.shape[0])
        
        # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Å–∏–¥—è—á–µ–≥–æ –ø–æ–ª–æ–∂–µ–Ω–∏—è
        if posture_type == PostureType.SITTING:
            messages = analyze_sitting_posture(keypoints, image.shape[:2])
            
            if messages:
                report = [
                    "**–ü—Ä–æ–±–ª–µ–º—ã —Å –æ—Å–∞–Ω–∫–æ–π (—Å–∏–¥—è):**",
                    *messages,
                    "\n**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**",
                    "‚Ä¢ –ü–æ–¥–Ω–∏–º–∏—Ç–µ —ç–∫—Ä–∞–Ω –Ω–∞ —É—Ä–æ–≤–µ–Ω—å –≥–ª–∞–∑",
                    "‚Ä¢ –û–±–æ–ø—Ä–∏—Ç–µ—Å—å –Ω–∞ —Å–ø–∏–Ω–∫—É –∫—Ä–µ—Å–ª–∞",
                    "‚Ä¢ –ü–æ—Å—Ç–∞–≤—å—Ç–µ –Ω–æ–≥–∏ –Ω–∞ –ø–æ–ª"
                ]
            else:
                report = ["**–û—Ç–ª–∏—á–Ω–∞—è –æ—Å–∞–Ω–∫–∞!**", "–í—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å–∏–¥–∏—Ç–µ –∑–∞ —Å—Ç–æ–ª–æ–º"]
        else:
            report = ["**–í—ã —Å—Ç–æ–∏—Ç–µ**", "–î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ—Å–∞–Ω–∫–∏ —Å—è–¥—å—Ç–µ –∑–∞ —Å—Ç–æ–ª"]
        
        return annotated_image, "\n".join(report), posture_type
        
    except Exception as e:
        return image, f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}", PostureType.UNKNOWN

def main():
    st.set_page_config(layout="wide")
    st.title("üì∑ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –æ—Å–∞–Ω–∫–∏ –¥–ª—è –æ—Ñ–∏—Å–Ω—ã—Ö —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
    if 'camera_on' not in st.session_state:
        st.session_state.camera_on = False
    
    # –ö–Ω–æ–ø–∫–∞ –≤–∫–ª—é—á–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã
    if not st.session_state.camera_on:
        st.markdown("""
        ## –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:
        1. –°—è–¥—å—Ç–µ –∑–∞ —Ä–∞–±–æ—á–∏–π —Å—Ç–æ–ª –∫–∞–∫ –æ–±—ã—á–Ω–æ
        2. –í–∫–ª—é—á–∏—Ç–µ –∫–∞–º–µ—Ä—É
        3. –†–∞—Å–ø–æ–ª–æ–∂–∏—Ç–µ—Å—å —Ç–∞–∫, —á—Ç–æ–±—ã –±—ã–ª–æ –≤–∏–¥–Ω–æ –≤–∞—à–∏ –ø–ª–µ—á–∏ –∏ –≥–æ–ª–æ–≤—É
        """)
        
        if st.button("–í–∫–ª—é—á–∏—Ç—å –∫–∞–º–µ—Ä—É", type="primary"):
            st.session_state.camera_on = True
            st.rerun()
        return
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("–í–∞—à–∞ –ø–æ–∑–∞")
        frame_placeholder = st.empty()
    
    with col2:
        st.header("–ê–Ω–∞–ª–∏–∑ –æ—Å–∞–Ω–∫–∏")
        status_placeholder = st.empty()
        
        if st.button("–í—ã–∫–ª—é—á–∏—Ç—å –∫–∞–º–µ—Ä—É"):
            st.session_state.camera_on = False
            st.rerun()
            return
    
    # –†–∞–±–æ—Ç–∞ —Å –∫–∞–º–µ—Ä–æ–π
    cap = cv2.VideoCapture(0)
    try:
        while st.session_state.camera_on:
            ret, frame = cap.read()
            if not ret:
                st.error("–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –∫–∞–º–µ—Ä–µ")
                break
            
            # –ê–Ω–∞–ª–∏–∑ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            analyzed, status, _ = analyze_posture(frame)
            frame_placeholder.image(analyzed, channels="BGR")
            status_placeholder.markdown(status)
            
            time.sleep(0.1)
    finally:
        cap.release()

if __name__ == "__main__":
    main()