import streamlit as st
import cv2
import numpy as np
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase
import mediapipe as mp
import av
from collections import deque

mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils

class PoseProcessor(VideoProcessorBase):
    def __init__(self):
        self.pose = mp_pose.Pose(min_detection_confidence=0.5,
                                min_tracking_confidence=0.5,
                                model_complexity=1)
        self.status_deque = deque(maxlen=1)

    def analyze_posture(self, landmarks, image_shape):
        h, w, _ = image_shape
        left_shoulder = landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]
        right_shoulder = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]
        left_hip = landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP]
        right_hip = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP]
        left_ear = landmarks.landmark[mp_pose.PoseLandmark.LEFT_EAR]
        right_ear = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EAR]
        nose = landmarks.landmark[mp_pose.PoseLandmark.NOSE]

        sitting = left_hip.y < left_shoulder.y + 0.1 or right_hip.y < right_shoulder.y + 0.1

        messages = []

        head_forward = (left_ear.y > left_shoulder.y + 0.1 or right_ear.y > right_shoulder.y + 0.1) and \
                       (nose.y > left_shoulder.y or nose.y > right_shoulder.y)
        if head_forward:
            messages.append("â€¢ Head tilted forward (text neck) (é ­ãŒå‰ã«å‚¾ã„ã¦ã„ã‚‹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒã‚¯ï¼‰)")

        shoulders_rounded = left_shoulder.x > left_hip.x + 0.05 or right_shoulder.x < right_hip.x - 0.05
        if shoulders_rounded:
            messages.append("â€¢ Rounded shoulders (è‚©ãŒä¸¸ã¾ã£ã¦ã„ã‚‹)")

        shoulder_diff = abs(left_shoulder.y - right_shoulder.y)
        hip_diff = abs(left_hip.y - right_hip.y)
        if shoulder_diff > 0.05 or hip_diff > 0.05:
            messages.append("â€¢ Side tilt (asymmetrical posture) (ä½“ãŒå‚¾ã„ã¦ã„ã‚‹ï¼ˆéå¯¾ç§°ãªå§¿å‹¢ï¼‰)")

        if sitting and (left_hip.y < left_shoulder.y + 0.15 or right_hip.y < right_shoulder.y + 0.15):
            messages.append("â€¢ Pelvis tilted forward (while sitting) (éª¨ç›¤ãŒå‰ã«å‚¾ã„ã¦ã„ã‚‹ï¼ˆåº§ã£ã¦ã„ã‚‹æ™‚ï¼‰)")

        if messages:
            report = [
                f"**{'Sitting' if sitting else 'Standing'} - posture issues detected ({'åº§ã£ã¦ã„ã‚‹' if sitting else 'ç«‹ã£ã¦ã„ã‚‹'} - å§¿å‹¢ã®å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ):**",
                *messages,
                "\n**Recommendations (ã‚¢ãƒ‰ãƒã‚¤ã‚¹):**",
                "â€¢ Keep your head straight, ears should be over shoulders (é ­ã‚’ã¾ã£ã™ãã«ä¿ã¡ã€è€³ã¯è‚©ã®ä¸Šã«æ¥ã‚‹ã‚ˆã†ã«)",
                "â€¢ Pull your shoulders back and down (è‚©ã‚’å¾Œã‚ã«å¼•ãä¸‹ã’ã‚‹)",
                "â€¢ Keep your back straight, avoid side tilts (èƒŒä¸­ã‚’ã¾ã£ã™ãã«ä¿ã¡ã€æ¨ªã«å‚¾ã‹ãªã„ã‚ˆã†ã«)",
                "â€¢ When sitting, rest on your sitting bones (åº§ã£ã¦ã„ã‚‹æ™‚ã¯åéª¨ã§æ”¯ãˆã‚‹)"
            ]
        else:
            report = [
                f"**Excellent posture ({'sitting' if sitting else 'standing'})! (ç´ æ™´ã‚‰ã—ã„å§¿å‹¢ï¼ˆ{'åº§ã£ã¦ã„ã‚‹' if sitting else 'ç«‹ã£ã¦ã„ã‚‹'}ï¼‰!)**",
                "All key points are in correct position. (ã™ã¹ã¦ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆãŒæ­£ã—ã„ä½ç½®ã«ã‚ã‚Šã¾ã™)",
                "\n**Tip (ãƒ’ãƒ³ãƒˆ):**",
                "â€¢ Continue to monitor your posture throughout the day (ä¸€æ—¥ä¸­å§¿å‹¢ã«æ°—ã‚’é…ã‚Šç¶šã‘ã¾ã—ã‚‡ã†)"
            ]

        return "\n\n".join(report)

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        results = self.pose.process(img_rgb)
        if results.pose_landmarks:
            mp_drawing.draw_landmarks(
                img, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),
                mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)
            )
            status = self.analyze_posture(results.pose_landmarks, img.shape)
            self.status_deque.append(status)
        else:
            self.status_deque.append("Key points not detected (ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ)")

        return av.VideoFrame.from_ndarray(img, format="bgr24")

def main():
    st.set_page_config(layout="wide")
    st.title("ğŸ“· Posture Analyzer with Web Camera (WebRTC) (ã‚¦ã‚§ãƒ–ã‚«ãƒ¡ãƒ©ã‚’ä½¿ã£ãŸå§¿å‹¢åˆ†æ)")
    st.write("This app analyzes your posture in real time using your browser's camera. (ã“ã®ã‚¢ãƒ—ãƒªã¯ãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚«ãƒ¡ãƒ©ã‚’ä½¿ã£ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å§¿å‹¢ã‚’åˆ†æã—ã¾ã™)")

    webrtc_ctx = webrtc_streamer(
        key="pose-analyzer",
        video_processor_factory=PoseProcessor,
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True
    )

    if webrtc_ctx.video_processor:
        if webrtc_ctx.video_processor.status_deque:
            analysis_text = webrtc_ctx.video_processor.status_deque[-1]
        else:
            analysis_text = "Waiting for video and analysis... (å‹•ç”»ã¨åˆ†æã‚’å¾…ã£ã¦ã„ã¾ã™...)"

        st.markdown("### Posture Analysis (å§¿å‹¢åˆ†æ):")
        st.markdown(analysis_text)

if __name__ == "__main__":
    main()